# CVE-2020-17087

[Here](https://bugs.chromium.org/p/project-zero/issues/detail?id=2104) is the original analysis of the vulnerability in question. 
A brief summary, the vulnerable function takes as input an array of bytes and outputs their hex representation in unicode. The hex encoded bytes are separated by space (0x20)

For example:
```
user input                      : 0                 129
output buffer (vulnerable_chunk): 30 00 30 00 20 00 38 00 31 00 20 00
```

With regards to the vulnerability itself, the problem exists in the output buffer (vulnerable_chunk) size calculation:
```c
vulnerable_chunk_size = (user_controlled_size*6)%65536;
vulnerable_chunk = AllocateMemory(vulnerable_chunk_size);
convert_to_hex(vulnerable_chunk, user_input, user_controlled_size*6)
```

As we can see, the vulnerable_chunk_size is 16 bit and as such the result of the multiplication gets truncated when the user provided size is big enough.
The problem is that the truncated multiplication result is used for the buffer allocation, but the non-truncated multiplication result is used for the actual procesisng of the data.

# Exploitaton:

Before getitng started, it is noted that all the code references/tests were performed on Windows 10 2004 x64.

To exploit this overflow, we will utilize the technique described in the "Limited control over the overflow data" chapter.

The challenges we have in this instance:

**1. Pick a vulnerable chunk size that allows us to overflow a few bytes of the victim data entry**

The following script can be used to identify some potential options:

```python
for vulnerable_chunk_size in range(0x10, 0x9999):
    overflow_size = 0x10000 + vulnerable_chunk_size
    request_size = overflow_size // 6

    #see the options of potentially overflowing 1-4 bytes of the flink (chunks are 16 bytes aligned at least for x64)
    #overflow_size should also be multiple of six so we shouldn't get a whole lot of 1,3 bytes
    if overflow_size%6!=0 or not (0<overflow_size%16<5):
        continue

    print(f"request_size={request_size:#x}, total_overflow_size={overflow_size:#x}, vulnerable_chunk_size={vulnerable_chunk_size:#x}, overwritten_bytes={overflow_size%8}")
```

We also know that all the allocators (assuming segment heap) should be capable of allocating over 64k of contiguous memory (through the RtlpHpLfhSubsegmentCreate, RtlpHpVsSubsegmentCreate, the theoretical LFH max: 0xf0000, VS max: 0x40000, the rest are straightforward)

So now after processing a little bit more the results, we have that:

a) *LFH*

Even though several sizes appear to fall within the LFH range, only the 0x152 (or 0x170 bucket) is usable, since it's the only one that has its chunks properly aligned to satisfy the equation:
```c
chunk_170_x_address + 0x10150 == chunk_170_y_address

chunk_170_x_address=vulnerable chunk
chunk_170_y_address=victim entry
```

(0x10152 is the overflow size, the equation above guarantees that we can overflow the Flink of the victim data entry)

b) *VS*

We have more usable options here. The script created to identify potential sizes is provided at the end of this document.

c) *Rest*

Unfortunately, based on the numbers it looks like the rest of the allocators cannot be used on their own for both vulnerable chunk and victim entries. One option could be the combination of allocators e.g. segment/VS but we have to be careful of guard pages.

**2. 64k memory of pool memory gets corrupted + the VULNERABLE_CHUNK gets freed after the overflow**

In this situation we have the 64k of memory following the vulnerable chunk overwritten with random data after the overflow. That will be a big problem if some code relies on any of those chunks and attempts to use it during the exploitation.
Since we will have control over the 64k of memory, we mostly want to avoid the pool allocator accessing the corrupted chunks. 
As we have seen in the previous point, we can use the LFH or VS allocators to control the pool memory. We will see how the situation described here applies to each of them.

*a) LFH*

   In LFH, there is not much risk of metadata corruption and the pool header of the chunks remains mostly unused. 
   We are corrupting the LFH subsegment if the vulnerable chunk and the victim chunk fall within different subsegments, but this doesn't appear to cause any problems if we fully control the victim subsegment.
   Now to the logistics, even though we have a usable size (0x152) for both vulnerable chunk/victim entry, we are still missing the "Corrupted Data Entries Identification" technique discussed in the overflow exploitation techniques.
   So this is not a viable option for now.

*b) VS*

   Initially, i assumed that the free chunk coalescing procedure of the VS allocator would be a big problem and thus ruled this option out.
   
   It turns out the coalescing function wasn't too much of a problem. The biggest problem appears to be related to the operations performed on the red/black tree that holds the free chunks.
   How that relates to our situation, even when we have control over a contiguous area in a VS subsegment we will most likely still end up with small free chunks within that area.
   This should be a direct consequence of having the PageAlignLargeAllocs flag in the VsContext->Config enabled. (see [here](https://speakerdeck.com/scwuaptx/windows-kernel-heap-segment-heap-in-windows-kernel-part-1) for more info on PageAlignLargeAllocs).

   For example, after spraying the pool with 0x3b0 size chunks we get the following layout:

```
1: kd> !pool FFFFDA06B51C0000
Pool page ffffda06b51c0000 region is Nonpaged pool
*ffffda06b51c0000 size:  3c0 previous size:    0  (Allocated) *ANOR
 ffffda06b51c03d0 size:  3c0 previous size:    0  (Allocated)  ANOR
 ffffda06b51c07a0 size:  3c0 previous size:    0  (Allocated)  ANOR
 ffffda06b51c0b70 size:  3c0 previous size:    0  (Allocated)  ANOR
 ffffda06b51c0f30 size:   b0 previous size:    0  (Free)       )..G

1: kd> !pool FFFFDA06B51C0000+1000
Pool page ffffda06b51c1000 region is Nonpaged pool
*ffffda06b51c1000 size:  3c0 previous size:    0  (Allocated) *ANOR
 ffffda06b51c13d0 size:  3c0 previous size:    0  (Allocated)  ANOR
 ffffda06b51c17a0 size:  3c0 previous size:    0  (Allocated)  ANOR
 ffffda06b51c1b70 size:  3c0 previous size:    0  (Allocated)  ANOR
 ffffda06b51c1f30 size:   b0 previous size:    0  (Free)       )..G
```

   So chunks located at ffffda06b51c0f30 and ffffda06b51c1f30 will be added as nodes to the free chunk tree. Now after the overflow, these nodes will get corrupted.
   Even though there is no risk of having those nodes removed from the tree for an allocation (e.g. assuming their sizes fall within the LFH range), there are still operations on the tree like rotations/additions of new chunks that might trip on the corrupted nodes.
   Two solutions around this issue:
   1. Pick a chunk size that completely fills the page and doesn't leave room for a free chunk.
   That should be the ideal option, but this doesn't look possible given our restriction in the overflow sizes. Edit: that's not accurate, see the approach we followed on the vuln_driver_all0c exploitation

   1. Pick a free chunk size that is not expected to cause too many problems.
   For example, free chunk sizes that are frequently created should be avoided, since they will most likely cause activity in the corrupted branch of the tree.
   We can find some of those sizes by setting the following breakpoint in windbg:
   
   ```
   bp nt!RtlpHpVsChunkSplit+40 ".if (@r15-@r9 < 0x22) {? (@r15-@r9)*0x10;gc} .else {gc}"
   ```

   Finally, by enabling the dynamic lookaside for the vulnerable chunk size before freeing it, we can skip a big part of the free function logic including the chunk coalescing.
   By enabling the dynamic lookaside we also decrease the probability of causing activity near/at the corrupted branch of the tree since for example the deallocation of ffffda06b51c1000 will normally cause the removal of ffffda06b51c0f30 chunk from the tree before merging the two. 


**3. Creating the right pool layout**

So our goal is to create a pool layout that would allow us to overflow just the Flink of the victim data entry after the 64k overflow.
The approach followed here is the following:

*a. Empty the FreeChunkTree by allocating a big number of small chunks*

*b. Add big subsegments into the FreeChunkTree.*

When the VS allocator cannot fulfill a request with the already available chunks, a new subsegment will be allocated. The size of the newly created subsegment will be derived by the missed allocation request size.
When the missed allocation size is smaller than 0x8000 then the subsegment would be 0x10000 bytes with a guard page added at the end. Since the overflow size is over 0x10000, this option is not usable.

When the allocation size is greater than 0x8000 the subsegment would be 0x20000 bytes which allow us to proceed with the exploitation of the issue. These are the subsegments we refer to as big here.

It is noted that there is another code path that should allow the allocation of 0x40000-byte subsegments but as far as i can tell it requires an allocation size greater than 0x10000 which falls outside the range of VS allocator (i.e. segment allocator). So it looks like a dead code but it would increase the reliability if we could somehow allocate them.

*c. Reclaim part of the previously released big subsegments with chunks having the vulnerable chunk size.*

It shouldn't take long to get the first big subsegment back with vulnerable_chunk_size allocations considering that the free chunk tree should be mostly empty. In the poc (and after a quick sampling) we make 48 allocations before assuming that the next chunk will fall within at least 64k of contiguous memory. 

With regards to the size of the vulnerable chunk, we want it to be big enough such as only one vulnerable chunk could fit in a page. This allow us to know that the vulnerable chunk user data will start at offset 0x10 of its allocation page and thus allow us to easily calculate where we should place the victim entry for proper alignment. (this appears to be one of the convenient aspects of PageAlignLargeAllocs flag when it comes to exploit devevelopment)

The picked size is 0x8d2

*d. Fill the rest of the big subsegments with the victim entries. The victim entry size is 0x2d0.*

In the end, we want a layout similar to this:

```
1: kd> !pool ffff900d64eed010
Pool page ffff900d64eed010 region is Nonpaged pool
 ffff900d64eed000 size:  900 previous size:    0  (Allocated)  NpFr <-------- future vulnerable_chunk
 ffff900d64eed910 size:  2e0 previous size:    0  (Allocated)  NpFr
 ffff900d64eedc00 size:  2e0 previous size:    0  (Allocated)  NpFr
 ffff900d64eedee0 size:  120 previous size:    0  (Free)       .sc.

 1: kd> !pool ffff900d64eed010+10000
Pool page ffff900d64efd010 region is Nonpaged pool
*ffff900d64efd000 size:  2e0 previous size:    0  (Allocated) *NpFr
 ffff900d64efd2f0 size:  2e0 previous size:    0  (Allocated)  NpFr
 ffff900d64efd5e0 size:  2e0 previous size:    0  (Allocated)  NpFr
 ffff900d64efd8d0 size:  2e0 previous size:    0  (Allocated)  NpFr <-------- victim entry
 ffff900d64efdbc0 size:  2e0 previous size:    0  (Allocated)  NpFr
 ffff900d64efdea0 size:  140 previous size:    0  (Free)       .sc.
 ```

*e. Spray the big pool and create the cover entry in the victim pipes*

Let's say we are planning to have the vulnerable chunk fall into the chunk located at ffff900d64eed000. We know that after the overflow the first two bytes of the data entry found at ffff900d64efd8d0 will get overwritten with 0x0020
Our goal here is to make sure that the redirected/undercover entry address will contain our forged data entry.

Let's say the cover data entry address is ffff900d64fa5000 (this will be the flink of the victim data entry at ffff900d64efd8d0), then we want the ffff900d64fa0020 (undercover entry) to contain the forged data entry.
Spraying using the segment allocator seems like a good option. 

We also make sure a valid forged data entry is formed even when the original entry address is ffff900d64fa0000 (after the overflow, the undercover entry will land into the data entry header of the cover entry similar to al20c). This allow us to slightly increase the reliability of the exploit. In the cost of bigger memory footprint, we could have also made use of the segment allocator with sizes that yield 0x10000 aligned allocations. That would increase the reliability of this stage even further.

---------------------------------

We can then proceed normally to establish the arbitrary read/write primitives.

It is noted that the poc provided here was tested on Windows 2004/20H2 x64. It should work on 1709+ versions without too many changes (probably just the WARMUP_NUMBER). 

For best reliability, try to run it after the system has booted up (we don't want overly active kernel memory activity to mess with our pool layout). After successfully overwriting our token we probably want to fix the FreeChunkTree bad branch with the overwritten nodes since it may explode at some point.

--------------------------------

The following script was used to calculate the vulnerable chunk and victim chunk sizes (it should be a bit buggy):

```python
def get_chunk_size(request_size):
    rounded_size = (request_size+0xf)//0x10*0x10
    if rounded_size <= 0xfe0:
        if rounded_size<=0x3f0:
            alignment = 0x10
        elif rounded_size <= 0x7f0:
            alignment = 0x40
        else:
            alignment = 0x80
        chunk_size = ((rounded_size+0x10)+alignment-1)&~(alignment-1)
        if rounded_size>=0x1f0: #add VS header in case the size is not serviced by LFH
            chunk_size+=0x10
        
        return chunk_size
        
    #non LFH and small VS
    chunk_size = rounded_size
    if rounded_size & 0xff0 > 0xFC0:
        chunk_size = (rounded_size + 0xfff)&~0xfff
        
    return chunk_size


def get_remaining_len(first_actual_chunk_size, second_actual_chunk_size):
    len = (0x1000-0x20-(first_actual_chunk_size-0x10))%second_actual_chunk_size
    if len == 0:
        return second_actual_chunk_size
    if len == 0x10:
        return 0
    return len

#vulnerable_chunk_req_size should fall within the dynamic lookaside range
for vulnerable_chunk_req_size in range(0x1f0, 0xf80):
    vulnerable_chunk_actual_size = get_chunk_size(vulnerable_chunk_req_size)
    overflow_size = 0x10000 + vulnerable_chunk_req_size
    
    if overflow_size%6 != 0 or overflow_size%0x10!=2:
        continue
    
    prev_target_actual_size = 0
    for victim_chunk_req_size in range(0x1f0, 0xf80, 0x10):
        victim_chunk_actual_size = get_chunk_size(victim_chunk_req_size)
        if victim_chunk_actual_size == prev_target_actual_size:
            continue
        if victim_chunk_actual_size*2>=0x1000-0x20:
            continue
            
        prev_target_actual_size = victim_chunk_actual_size
        
        #check for Flink alignment. note: the vulnerable chunk data start at offset 0x10 (after pool header)
        #we also assume chunks have the following structure to make calculations easier: |POOL_HEADER|USER_DATA|VS_HEADER|
        if (vulnerable_chunk_req_size+0x10-2)%victim_chunk_actual_size != 0x10:
            continue
        
        #we don't want the victim chunk to span over two pages
        if vulnerable_chunk_req_size+victim_chunk_actual_size>=0x1000-0x20:
            continue

        #we want only one vulnerable chunk per page
        if 0x1000-0x20-(vulnerable_chunk_actual_size-0x10)>=vulnerable_chunk_actual_size:
           continue
        
        #we want the free chunks created by the vulnerable and victim chunks to fall within the LFH to avoid reallocation.
        if get_remaining_len(vulnerable_chunk_actual_size, victim_chunk_actual_size)>0x210:
            continue  
        if get_remaining_len(victim_chunk_actual_size, victim_chunk_actual_size)>0x210:
            continue
            
        print(f"vuln chunk size: {overflow_size//6:#x}, vulnerable_chunk_req_size={vulnerable_chunk_req_size:#x}, victim_chunk_req_size: {victim_chunk_req_size:#x}, fragment sizes: ", hex(get_remaining_len(victim_chunk_actual_size, victim_chunk_actual_size)),hex(get_remaining_len(vulnerable_chunk_actual_size, victim_chunk_actual_size)))
```
